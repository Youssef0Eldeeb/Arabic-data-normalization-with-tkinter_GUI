{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839fc93f",
   "metadata": {},
   "source": [
    "## Normalization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8369914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmentaion\n",
    "\n",
    "def segmentation(txt):\n",
    "    import nltk\n",
    "    punkt = nltk.data.load(r'./tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "        \n",
    "    segmentedText = punkt.tokenize(comingText)\n",
    "    \n",
    "    with open(r'./Output_Files/Segmented_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{segmentedText}')\n",
    "\n",
    "\n",
    "# segmentation('society2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a70696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "def tokenization(txt):\n",
    "    \n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "    \n",
    "    import nltk\n",
    "    tokenizedText = nltk.word_tokenize(comingText)\n",
    "    \n",
    "    with open(r'./Output_Files/Tokenized_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{tokenizedText}')\n",
    "    \n",
    "#tokenization('algeria.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56fee8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword Removing\n",
    "\n",
    "def stopwordRemoving(txt):\n",
    "    \n",
    "    stopWordList = open(\"arabic-stop-words.txt\").read().splitlines()\n",
    "    \n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "    \n",
    "    SWCleanText = []\n",
    "    tokensText = comingText.split(' ')\n",
    "    for token in tokensText:\n",
    "        if token not in stopWordList:\n",
    "            SWCleanText.append(token)\n",
    "    resultText = ' '.join(SWCleanText)        \n",
    "    with open(r'./Output_Files/Stopword_Removed_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{resultText}')\n",
    "\n",
    "# stopwordRemoving('society2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8326178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuations Removing\n",
    "\n",
    "def punctuationsRemoving(txt):\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "    \n",
    "    ResultText = ''.join(c for c in comingText if not ud.category(c).startswith('P'))\n",
    "    \n",
    "    with open(r'./Output_Files/Punctuations_Removed_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{ResultText}')\n",
    "    \n",
    "# punctuationsRemoving(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede7196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISRI Stemmer (Root-based stemmer)\n",
    "\n",
    "def ISRI_Stemmer(txt):\n",
    "    import nltk\n",
    "    st = nltk.ISRIStemmer()\n",
    "    \n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "    \n",
    "    tokensText = comingText.split(' ')\n",
    "    \n",
    "    resultStem = ' '.join([st.stem(w) for w in tokensText])\n",
    "    \n",
    "    with open(r'./Output_Files/Root-Based_Stemming_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{resultStem}')\n",
    "\n",
    "# ISRI_Stemmer('society2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3bc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Light Stemmer (tashaphyne)\n",
    "\n",
    "def lightStemmer(txt):\n",
    "    stemCleanText = []\n",
    "    from tashaphyne.stemming import ArabicLightStemmer\n",
    "    lightStem = ArabicLightStemmer()\n",
    "    \n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "        \n",
    "    tokensText = comingText.split(' ')\n",
    "    for token in tokensText:\n",
    "        stem = lightStem.light_stem(token)\n",
    "        stemCleanText.append(stem)\n",
    "    \n",
    "    resultText = ' '.join(stemCleanText)\n",
    "    \n",
    "    with open(r'./Output_Files/Light_Stemming_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{resultText}')\n",
    "\n",
    "# lightStemmer('society2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85770ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All preprocessing steps (Normalization of data)\n",
    "\n",
    "def normalization(txt):\n",
    "    with open(f'{txt}','r') as f:\n",
    "        comingText = f.read()\n",
    "    #Tokenization\n",
    "    tokenText = comingText.split(' ')\n",
    "    \n",
    "    #StopWord Removing\n",
    "    SWList = []\n",
    "    stopWordList = open(\"arabic-stop-words.txt\").read().splitlines()\n",
    "    for token in tokenText:\n",
    "        if token not in stopWordList:\n",
    "            SWList.append(token)\n",
    "    SWText = ' '.join(SWList)\n",
    "    \n",
    "    # Punctuations Removing\n",
    "    import unicodedata as ud\n",
    "    puncText = ''.join(c for c in SWText if not ud.category(c).startswith('P'))\n",
    "    \n",
    "    # ISRI Stemmer (Root-based stemmer)\n",
    "    import nltk\n",
    "    st = nltk.ISRIStemmer()\n",
    "    tkText = puncText.split(' ')\n",
    "    result = ' '.join([st.stem(w) for w in tkText])\n",
    "    \n",
    "    with open(r'./Output_Files/Normalized_Text', 'w') as outputFile:\n",
    "        outputFile.write(f'{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e274e",
   "metadata": {},
   "source": [
    "## GUI with tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import tkinter.messagebox\n",
    "\n",
    "main = Tk()\n",
    "main.title('Home')\n",
    "main.geometry(\"500x300\")\n",
    "\n",
    "#---------- Entry (textbox)-----------------\n",
    "text = StringVar()\n",
    "# text.set(\"Enter your path\")\n",
    "textbox = Entry(main, textvariable=text)\n",
    "textbox.place(x=200 ,y=50)\n",
    "\n",
    "# call function when we click on entry box\n",
    "def click(*args):\n",
    "    if textbox.get() == 'Enter Path of Dataset:- ':\n",
    "        textbox.delete(0, 'end')\n",
    "    \n",
    "#call function when we leave entry box\n",
    "def leave(*args):\n",
    "    if textbox.get() == '':\n",
    "        textbox.delete(0, 'end')\n",
    "        textbox.insert(0, 'Enter Path of Dataset:- ')\n",
    "        main.focus()\n",
    "    else:\n",
    "        main.focus()\n",
    "    \n",
    "# Add text in Entry box\n",
    "textbox.insert(0, 'Enter Path of Dataset:- ')\n",
    "textbox.pack(pady=10)\n",
    "# Use bind method\n",
    "textbox.bind(\"<Button-1>\", click)\n",
    "textbox.bind(\"<Leave>\", leave)\n",
    "\n",
    "textbox.place(x=150 ,y=15)\n",
    "\n",
    "#------------Buttons-------------\n",
    "segmentationBtn = Button(main, text=\"Segmentation\",fg=\"Red\", command=lambda : segmentation(textbox.get()))\n",
    "tokenizationBtn = Button(main, text=\"Tokenization\",fg=\"Red\", command=lambda : tokenization(textbox.get()))\n",
    "stopwordRemovingBtn = Button(main, text=\"Stopword Removing\",fg=\"Red\", command=lambda : stopwordRemoving(textbox.get()))\n",
    "punctuationsRemovingBtn = Button(main, text=\"Punctuations Removing\",fg=\"Red\", command=lambda : punctuationsRemoving(textbox.get()))\n",
    "rootBasedStemmerBtn = Button(main, text=\"Root-Based Stemmer\",fg=\"Red\", command=lambda : ISRI_Stemmer(textbox.get()))\n",
    "lightStemmerBtn = Button(main, text=\"Light-Based Stemmer\",fg=\"Red\", command=lambda : lightStemmer(textbox.get()))\n",
    "normalizationBtn = Button(main, text=\"Normalization\",fg=\"Red\", command=lambda : normalization(textbox.get()))\n",
    "\n",
    "segmentationBtn.place(x=150 , y=80)\n",
    "tokenizationBtn.place(x=150 , y=110)\n",
    "stopwordRemovingBtn.place(x=150 , y=140)\n",
    "punctuationsRemovingBtn.place(x=150 , y=170)\n",
    "rootBasedStemmerBtn.place(x=150 , y=200)\n",
    "lightStemmerBtn .place(x=150 , y=230)\n",
    "normalizationBtn.place(x=150, y=269)\n",
    "\n",
    "\n",
    "main.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd552d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02e87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
